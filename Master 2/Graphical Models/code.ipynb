{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import VGAE\n",
    "\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "from torch_geometric.transforms import remove_isolated_nodes\n",
    "import torch_geometric.utils as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15 # top-K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "\n",
    "# Divise in train and test set\n",
    "train = dataset[:int(len(dataset)*.8)]\n",
    "test = dataset[int(len(dataset)*.8):]\n",
    "\n",
    "# Use the DataLoader class to create a batch iterator\n",
    "batch = 32\n",
    "train_loader = DataLoader(train, batch_size=batch, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pretained GNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(torch.nn.Module):\n",
    "    def __init__(self, num_features=dataset.num_features, num_classes=dataset.num_classes):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 20)\n",
    "        self.conv2 = GCNConv(20, 20)\n",
    "        self.conv3 = GCNConv(20, 20)\n",
    "        self.fc = Linear(20*3, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x1 = self.conv1(x, edge_index).relu()\n",
    "        x2 = self.conv2(x1, edge_index).relu()\n",
    "        x3 = self.conv3(x2, edge_index).relu()\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = global_max_pool(x, data.batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model_backup = GraphNet()\n",
    "print(model_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  copy.deepcopy(model_backup)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 1000\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy(pred_y, y):\n",
    "    return (pred_y == y).sum() / len(y)\n",
    "\n",
    "# List for plots\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize variables for tracking loss and accuracy\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    # Iterate over the batches in the train_loader\n",
    "    for data in train_loader:\n",
    "        \n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, data.y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        pred_y = output.argmax(dim=1)\n",
    "        acc = accuracy(pred_y, data.y)\n",
    "        \n",
    "        # Update the total accuracy\n",
    "        total_accuracy += acc.item()\n",
    "    \n",
    "    # Calculate the average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_accuracy = total_accuracy / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    accuracies.append(avg_accuracy)\n",
    "    \n",
    "    # Print the epoch number, loss, and accuracy\n",
    "    if epoch == num_epochs-1:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, Accuracy = {avg_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Plot the loss and accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss for GNN\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies)\n",
    "plt.title(\"Accuracy for GNN\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained GNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables for tracking loss and accuracy\n",
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "\n",
    "# Iterate over the batches in the test_loader\n",
    "for graph in test_loader:\n",
    "    # Forward pass\n",
    "    output = model(graph)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(output, graph.y)\n",
    "    \n",
    "    # Update the total loss\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    pred_y = output.argmax(dim=1)\n",
    "    acc = accuracy(pred_y, graph.y)\n",
    "    \n",
    "    # Update the total accuracy\n",
    "    total_accuracy += acc.item()\n",
    "\n",
    "# Calculate the average loss and accuracy for the test set\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "avg_accuracy = total_accuracy / len(test_loader)\n",
    "\n",
    "# Print the average loss and accuracy\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distillation():\n",
    "    def __init__(self, model, criterion=torch.nn.CrossEntropyLoss(), K=K, connected=False):\n",
    "        self.K = K*2\n",
    "        self.connected = connected\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def process(self, graphs):\n",
    "        subgraphs = []\n",
    "        for graph in graphs:\n",
    "            subgraph = self.one_graph(graph)\n",
    "            subgraphs.append(subgraph)\n",
    "        return subgraphs\n",
    "\n",
    "    \n",
    "    def one_graph(self, graph):\n",
    "        Ec_sorted = sorted(self.get_causal_contributions(graph), key=lambda x: x[1], reverse=False)\n",
    "        Gs = copy.copy(graph)\n",
    "        Gs, Es = self.apply_weights(Gs, Ec_sorted)\n",
    "        Es_sorted = sorted(Es, key=lambda x: x[1], reverse=False)\n",
    "        Gs = self.distill(Gs, Es_sorted)\n",
    "        return Gs\n",
    "\n",
    "\n",
    "    ###### Process functions ######\n",
    "    def get_causal_contributions(self, graph):\n",
    "        \"\"\"\n",
    "        Get the causal contributions for all edges in the graph\n",
    "\n",
    "        Args:\n",
    "            graph (torch_geometric.data.Data): The graph\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples containing the edge and its causal contribution\n",
    "        \"\"\"\n",
    "        causal_contributions = []\n",
    "        for edge in graph.edge_index.t().tolist():\n",
    "            delta = self.causal_contribution(graph, edge)\n",
    "            causal_contributions.append((edge, delta))\n",
    "        return causal_contributions\n",
    "\n",
    "    def apply_weights(self, graph, edges):\n",
    "        \"\"\"\n",
    "        Calculate the weights for the edges\n",
    "\n",
    "        Args:\n",
    "            graph (torch_geometric.data.Data): The graph\n",
    "            edges (list): A list of edges\n",
    "\n",
    "        Returns:\n",
    "            torch_geometric.data.Data: The graph with the weights applied\n",
    "            list: A list of tuples containing the edge and its weight\n",
    "        \"\"\"\n",
    "        weights_list = []\n",
    "        for edge, causal_contribution in edges:\n",
    "            graph_ = self.remove_edge(graph, edge)\n",
    "            if graph_.edge_index.size(1) == 0:\n",
    "                break\n",
    "            if self.connected:\n",
    "                pass\n",
    "            loss = self.criterion(self.model(graph), graph.y)\n",
    "            loss_ = self.criterion(self.model(graph_), graph_.y)\n",
    "            if  loss_ > loss:\n",
    "                weight = (loss - loss_).item()\n",
    "                weights_list.append((edge, weight))\n",
    "            else:\n",
    "                graph = copy.deepcopy(graph_)\n",
    "                if graph.edge_index.size(1) <= K: # if the graph has not enough important edges \n",
    "                    return graph, weights_list\n",
    "        return graph, weights_list\n",
    "    \n",
    "    def distill(self, graph, edges):\n",
    "        \"\"\"\n",
    "        Distill the subgraph with top-k most relevant edges\n",
    "\n",
    "        Args:\n",
    "            graph (torch_geometric.data.Data): The graph\n",
    "            edges (list): A list of edges sorted by weight\n",
    "\n",
    "        Returns:\n",
    "            torch_geometric.data.Data: The distilled subgraph\n",
    "        \"\"\"\n",
    "        for edge, weight in edges:\n",
    "            if len(graph.edge_index.t()) > self.K:\n",
    "                graph = self.remove_edge(graph, edge)\n",
    "                if self.connected:\n",
    "                    pass\n",
    "        return graph\n",
    "\n",
    "    ###### Helper functions ######\n",
    "    def remove_edge(self, graph, edge):\n",
    "        \"\"\"\n",
    "        Remove an edge from the graph\n",
    "\n",
    "        Args:\n",
    "            graph (torch_geometric.data.Data): The graph\n",
    "            edge (tuple): The edge to remove\n",
    "\n",
    "        Returns:\n",
    "            torch_geometric.data.Data: The graph without the edge\n",
    "        \"\"\"\n",
    "        edges = []\n",
    "        for e in graph.edge_index.t().tolist():\n",
    "            if e != edge and e != edge[::-1]: # Remove the edge and its reverse because the graph is undirected\n",
    "                edges.append(e)\n",
    "        graph_ = copy.deepcopy(graph)\n",
    "        graph_.edge_index = torch.tensor(edges).t().contiguous()\n",
    "        return graph_\n",
    "    \n",
    "    def causal_contribution(self, graph, edge):\n",
    "        \"\"\"\n",
    "        Calculate the causal contribution of an edge\n",
    "\n",
    "        Args:\n",
    "            graph (torch_geometric.data.Data): The graph\n",
    "            edge (tuple): The edge to calculate the causal contribution for\n",
    "\n",
    "        Returns:\n",
    "            float: The causal contribution of the edge\n",
    "        \"\"\"\n",
    "        loss = self.criterion(self.model(graph), graph.y)\n",
    "        graph_ = self.remove_edge(graph, edge)\n",
    "        subloss = self.criterion(self.model(graph_), graph_.y)\n",
    "        return (subloss - loss ).item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_train = Distillation(model).process(train)\n",
    "ground_truth_test = Distillation(model).process(test)\n",
    "ground_truth_train_loader = DataLoader(ground_truth_train, batch_size=batch, shuffle=True)\n",
    "ground_truth_test_loader = DataLoader(ground_truth_test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generative Model as an Explainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "    def forward(self, z, edge_index=None):\n",
    "        if edge_index is None:\n",
    "            return torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        else:\n",
    "            return torch.sigmoid((z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1))\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    \"\"\"Encoder for the variational graph autoencoder.\"\"\"\n",
    "    def __init__(self, in_channels=dataset.num_features, out_channels=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2*out_channels)\n",
    "        self.conv2 = GCNConv(2*out_channels, 2*out_channels)\n",
    "        self.conv_mu = GCNConv(2*out_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(2*out_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n",
    "    \n",
    "class GVAE(torch.nn.Module):\n",
    "    \"\"\" Graph Variational Auto-Encoder.\"\"\"\n",
    "    def __init__(self, encoder, decoder=InnerProductDecoder()):\n",
    "        super(GVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        GVAE.reset_parameters(self)\n",
    "\n",
    "    #### GAE functions ####\n",
    "    def reset_parameters(self):\n",
    "        reset(self.encoder)\n",
    "        reset(self.decoder)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.encoder(x, edge_index)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index=None):\n",
    "        EPS = 1e-15\n",
    "        pos_loss = -torch.log(self.decoder(z, pos_edge_index) + EPS).mean()\n",
    "        if neg_edge_index is None:\n",
    "            neg_edge_index = utils.negative_sampling(pos_edge_index, z.size(0))\n",
    "        neg_loss = -torch.log(1 - self.decoder(z, neg_edge_index) + EPS).mean()\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "    #### VGAE add functions ####  \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logvar) * torch.exp(logvar)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def encode(self, x, edge_index):\n",
    "        mu, logvar = self.encoder(x, edge_index)\n",
    "        self.mu = mu\n",
    "        self.logvar = logvar\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z\n",
    "    \n",
    "    def kl_loss(self):\n",
    "        return -0.5 * torch.mean(torch.sum(1 + 2 * self.logvar - self.mu**2 - self.logvar.exp()**2, dim=1))\n",
    "       \n",
    "explainer_backup = GVAE(Encoder(), InnerProductDecoder())\n",
    "print(explainer_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer =  copy.deepcopy(explainer_backup)\n",
    "\n",
    "optimizer = torch.optim.Adam(explainer.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "# List for plots\n",
    "losses = []\n",
    "\n",
    "# Set the explainer to training mode\n",
    "explainer.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs+1):\n",
    "    # Initialize variables for tracking loss and accuracy\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over the batches in the train_loader\n",
    "    for graphs in ground_truth_train_loader:\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode the input\n",
    "        z = explainer.encode(graph.x, graph.edge_index)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = explainer.recon_loss(z, graph.edge_index)\n",
    "        loss = loss + (1 / graph.num_nodes) * explainer.kl_loss()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_loss = total_loss / len(ground_truth_train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Print the epoch number, loss, and accuracy\n",
    "    if epoch == num_epochs:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Plot the loss and accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss for VGAE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "explainer.eval()\n",
    "\n",
    "# Initialize variables for tracking loss and accuracy\n",
    "total_loss = 0\n",
    "\n",
    "# Iterate over the batches in the test_loader\n",
    "for graph in ground_truth_test_loader:\n",
    "    \n",
    "    # Encode the input\n",
    "    z = explainer.encode(graph.x, graph.edge_index)\n",
    "           \n",
    "    # Calculate the loss\n",
    "    loss = explainer.recon_loss(z, graph.edge_index)\n",
    "    loss = loss + (1 / graph.num_nodes) * explainer.kl_loss()\n",
    "    \n",
    "    # Update the total loss\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "# Calculate the average loss and accuracy for the test set\n",
    "avg_loss = total_loss / len(ground_truth_test_loader)\n",
    "\n",
    "# Print the average loss and accuracy\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the accuracy of _Gem_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for tracking loss and accuracy\n",
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "\n",
    "# Iterate over the batches in the test_loader\n",
    "for graph in test_loader:\n",
    "    \n",
    "    # Create the adjacency matrix\n",
    "    z = explainer.encode(graph.x, graph.edge_index)\n",
    "    A = explainer.decode(z)\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = copy.deepcopy(graph)\n",
    "    subgraph.edge_index = A.nonzero().t().contiguous()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(subgraph)\n",
    "\n",
    "    # Calculate the loss and the accuracy\n",
    "    loss = criterion(output, graph.y)\n",
    "    \n",
    "    # Update the total loss\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    pred_y = output.argmax(dim=1)\n",
    "    acc = accuracy(pred_y, graph.y)\n",
    "    \n",
    "    # Update the total accuracy\n",
    "    total_accuracy += acc.item()\n",
    "       \n",
    "# Calculate the average loss and accuracy for the test set\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "avg_accuracy = total_accuracy / len(test_loader)\n",
    "\n",
    "# Print the average loss and accuracy\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {avg_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
